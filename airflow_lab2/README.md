# Airflow Lab 2 — ML Pipeline Orchestration with Apache Airflow 3.x, Email Notifications & FastAPI Monitoring

## What This Project Does

This project implements an **end-to-end machine learning pipeline** orchestrated by **Apache Airflow 3.0.2**, running inside **Docker Compose**. The pipeline trains a Logistic Regression classifier on the Breast Cancer Wisconsin dataset, sends email notifications via Gmail SMTP on completion, and provides a real-time **FastAPI monitoring dashboard** that queries the Airflow REST API using JWT authentication.

This is a modified version of the original Airflow Lab 2 from [Prof. Ramin Mohammadi's MLOps course](https://github.com/raminmohammadi/MLOps/tree/main/Labs/Airflow_Labs/Lab_2). The key changes from the original lab are:

- **Docker Compose** instead of bare `pip install` — the original approach causes SIGSEGV crashes on modern macOS/ARM
- **Airflow 3.0.2** instead of 2.x — uses FAB auth manager and JWT-based authentication
- **FastAPI** replaces Flask for the monitoring API — adds auto-generated Swagger docs, structured JSON responses, and async endpoints
- **Airflow 3.x import paths** — operators moved from `airflow.operators.*` to `airflow.providers.standard.operators.*`


## Project Structure

```
airflow_lab2/
├── docker-compose.yaml          # All services: Postgres, Airflow, FastAPI
├── Dockerfile                    # Custom Airflow image with ML dependencies
├── requirements.txt              # Extra Python packages (pandas, scikit-learn, etc.)
├── .env                          # Credentials & config (edit before running)
├── dags/
│   ├── main.py                   # DAG: ML pipeline with 7 tasks
│   ├── fastapi_monitor.py        # Standalone FastAPI server for monitoring
│   ├── data/
│   │   └── breast_cancer.csv     # Dataset (569 samples, 30 features)
│   ├── model/                    # Auto-generated after first run (model.pkl, scaler.pkl)
│   ├── src/
│   │   ├── __init__.py
│   │   └── model_development.py  # ML functions: load, preprocess, split, train, evaluate
│   └── templates/
│       ├── success.html          # Rendered when pipeline succeeds
│       └── failure.html          # Rendered when pipeline fails
├── config/                       # Airflow config (auto-generated by Docker)
├── logs/                         # Airflow task logs (auto-generated)
```


## Architecture

```
┌──────────────────────────────────────────────────────────────────┐
│  Docker Compose Network                                          │
│                                                                  │
│  ┌────────────┐   ┌──────────────────┐   ┌───────────────────┐  │
│  │ PostgreSQL  │◄──│ Airflow API      │   │ DAG Processor     │  │
│  │ (metadata   │   │ Server           │   │ (parses DAG files │  │
│  │  database)  │   │ (UI on :8080)    │   │  every 30s)       │  │
│  └────────────┘   └──────────────────┘   └───────────────────┘  │
│                    ┌──────────────────┐   ┌───────────────────┐  │
│                    │ Scheduler        │   │ Triggerer         │  │
│                    │ (assigns tasks   │   │ (deferred tasks)  │  │
│                    │  to executor)    │   │                   │  │
│                    └──────────────────┘   └───────────────────┘  │
│                    ┌─────────────────────────────────────────┐   │
│                    │ FastAPI Monitor (port :8000)             │   │
│                    │  → Authenticates via POST /auth/token    │   │
│                    │  → Queries Airflow REST API with JWT     │   │
│                    │  → Serves status pages + JSON endpoints  │   │
│                    └─────────────────────────────────────────┘   │
└──────────────────────────────────────────────────────────────────┘
```


## ML Pipeline (DAG: `Airflow_Lab2`)

The DAG executes 7 tasks in sequence:

```
owner_task → load_data → data_preprocessing → separate_data_outputs → build_model → load_model → send_email
```

| Task | Operator | What It Does |
|---|---|---|
| `owner_task` | `BashOperator` | Echoes pipeline ownership info (demonstrates shell integration) |
| `load_data` | `PythonOperator` | Reads `breast_cancer.csv`, pushes DataFrame to XCom |
| `data_preprocessing` | `PythonOperator` | Checks for nulls, applies `StandardScaler` to 30 features, saves scaler |
| `separate_data_outputs` | `PythonOperator` | 80/20 stratified train-test split |
| `build_model` | `PythonOperator` | Trains `LogisticRegression(max_iter=1000)`, saves `model.pkl` |
| `load_model` | `PythonOperator` | Loads saved model, evaluates on test set, logs accuracy + classification report |
| `send_email` | `SmtpOperator` | Sends pipeline completion notification via Gmail SMTP |

Data is passed between tasks using **Airflow XCom** (cross-communication). Each task serializes its output as JSON and the next task pulls it.


## Prerequisites

- **Docker Desktop** with at least **4 GB memory** allocated (ideally 8 GB)
- **Docker Compose** v2.14.0 or newer
- **Gmail account** with 2-Step Verification enabled (for SMTP email notifications)
- Operating System: macOS, Linux, or Windows with WSL2


## Step-by-Step Setup

### 1. Clone or download this project

```bash
cd airflow_lab2
```

### 2. Create required directories

```bash
mkdir -p ./logs
```

### 3. Configure environment variables

Edit the `.env` file:

```bash
# .env
AIRFLOW_UID=50000

_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow

# ── UPDATE THESE with your Gmail credentials ──
SMTP_USER=your-actual-email@gmail.com
SMTP_PASSWORD=your-16-char-app-password
SMTP_MAIL_FROM=your-actual-email@gmail.com

NOTIFICATION_EMAIL_TO=others-actual-email@gmail.com

AIRFLOW_CONN_SMTP_DEFAULT=smtp://YOUR_EMAIL%40gmail.com:YOUR_APP_PASSWORD@smtp.gmail.com:587?from_email=YOUR_EMAIL&disable_ssl=true
```

**To get the SMTP App Password:**
1. Go to your Google Account → Security
2. Enable **2-Step Verification** if not already enabled
3. Visit https://myaccount.google.com/apppasswords
4. Select "Mail" → "Other (Airflow)" → Generate
5. Copy the 16-character password into `.env`

### 4. Build the custom Docker image

```bash
docker compose build
```

This builds a custom Airflow image on top of `apache/airflow:3.0.2` with our extra dependencies (`pandas`, `scikit-learn`, `uvicorn`, etc.) defined in `requirements.txt`.

### 5. Initialize the database and create the admin user

```bash
docker compose up airflow-init
```

Wait for it to finish. You should see:
```
airflow-init-1  | User "airflow" created with role "Admin"
airflow-init-1  | 3.0.2
airflow-init-1 exited with code 0
```

**`exited with code 0` is expected and correct** — the init container is a one-time job that runs DB migrations and creates the admin user, then exits.

### 6. Start all services

```bash
docker compose up -d
```

Wait ~45 seconds for all containers to become healthy:

```bash
docker ps
```

You should see 6 running containers: `postgres`, `airflow-apiserver`, `airflow-scheduler`, `airflow-dag-processor`, `airflow-triggerer`, and `fastapi-monitor`.

### 7. Access the services

| Service | URL | Credentials |
|---|---|---|
| Airflow UI | http://localhost:8080 | airflow / airflow |
| FastAPI Monitor | http://localhost:8000 | — |
| Swagger API Docs | http://localhost:8000/docs | — |
| Pipeline JSON Status | http://localhost:8000/api | — |
| Pipeline HTML Status | http://localhost:8000/status | — |
| Health Check | http://localhost:8000/health | — |

### 8. Trigger the ML pipeline

1. Go to http://localhost:8080, login with `airflow` / `airflow`
2. Find `Airflow_Lab2` in the DAG list
3. Toggle it **ON** (unpause switch on the left)
4. Click the **▶ play button** → "Trigger DAG"
5. Click the DAG name to see the graph view — tasks turn green as they complete

### 9. Monitor the results

After the DAG completes (~30-60 seconds), check:

```bash
# JSON status
curl http://localhost:8000/api

# Or visit in browser
open http://localhost:8000/status
```


## Key Technologies & Concepts Demonstrated

| Technology | How It's Used |
|---|---|
| **Apache Airflow 3.0.2** | Workflow orchestration — DAGs, operators, XCom, scheduling |
| **Docker Compose** | Multi-container deployment — Postgres, Airflow services, FastAPI |
| **FAB Auth Manager** | Airflow authentication — user management via Flask-AppBuilder |
| **JWT Authentication** | Token-based auth for Airflow REST API (`POST /auth/token`) |
| **PythonOperator** | Executing ML pipeline functions as Airflow tasks |
| **BashOperator** | Running shell commands within the pipeline |
| **SmtpOperator** | Sending email notifications via Gmail SMTP |
| **FastAPI** | Building the monitoring REST API with auto-generated Swagger docs |
| **scikit-learn** | Training a Logistic Regression model, StandardScaler, train/test split |
| **XCom** | Passing data (as serialized JSON) between Airflow tasks |
| **PostgreSQL** | Airflow metadata database (stores DAG runs, task states, XCom values) |


## Airflow 3.x vs 2.x — What Changed

If you're coming from Airflow 2.x, these are the breaking changes we encountered:

| What | Airflow 2.x | Airflow 3.x |
|---|---|---|
| DAG import | `from airflow import DAG` | `from airflow.sdk import DAG` |
| PythonOperator | `from airflow.operators.python import PythonOperator` | `from airflow.providers.standard.operators.python import PythonOperator` |
| BashOperator | `from airflow.operators.bash import BashOperator` | `from airflow.providers.standard.operators.bash import BashOperator` |
| EmailOperator | `from airflow.operators.email import EmailOperator` | `from airflow.providers.smtp.operators.smtp import SmtpOperator` |
| Auth manager | Built-in basic auth | FAB auth manager (requires explicit config) |
| API authentication | Basic auth headers | JWT tokens via `POST /auth/token` |
| Task callbacks | `on_failure_callback` works | **Not yet implemented** ([#44354](https://github.com/apache/airflow/issues/44354)) |
| JWT secret | Auto-managed | **Must be explicitly set** via `AIRFLOW__API_AUTH__JWT_SECRET` ([#59373](https://github.com/apache/airflow/issues/59373)) |


## Troubleshooting

### SIGSEGV crashes when running `airflow webserver` locally

Airflow 2.x/3.x has known segfault issues on macOS ARM. **Use Docker Compose instead of `pip install apache-airflow`.**

### `Signature verification failed` — tasks stuck in queued/failed

This is the most common Airflow 3.x Docker issue. Each container auto-generates its own JWT secret if you don't set one explicitly. The fix is to add this to `docker-compose.yaml` in the shared environment:

```yaml
AIRFLOW__API_AUTH__JWT_SECRET: 'your-shared-secret-here'
AIRFLOW__CORE__FERNET_KEY: 'your-fernet-key-here'
AIRFLOW__CORE__INTERNAL_API_SECRET_KEY: 'your-shared-secret-here'
```

All three must be set and consistent across all containers. After changing, you must do a full clean restart:

```bash
docker compose down --volumes --remove-orphans
docker compose up airflow-init
docker compose up -d
```

### `airflow-init exited with code 0`

This is **normal and expected**. The init container runs database migrations, creates the admin user, and exits. `code 0` = success.

### `airflow-dag-processor` shows `(unhealthy)`

Check if DAGs are parsing correctly:
```bash
docker compose logs airflow-dag-processor --tail 30
```

Look for `# Errors` column. If it shows `0`, the processor is working fine despite the health check status.

### `NotImplementedError: Haven't coded Task callback yet`

Task-level `on_failure_callback` and `on_success_callback` are **not yet implemented** in Airflow 3.0.x. Remove them from `default_args` and individual tasks. Use the `SmtpOperator` as a dedicated email task instead.

### `send_email` task fails

Verify your SMTP credentials in `.env`. Also ensure you've created a Gmail App Password (not your regular Google password). Test SMTP connectivity:
```bash
docker compose exec airflow-scheduler python -c "
import smtplib
s = smtplib.SMTP('smtp.gmail.com', 587)
s.starttls()
s.login('your-email@gmail.com', 'your-app-password')
print('SMTP OK')
s.quit()
"
```


## Cleanup

```bash
# Stop and remove all containers, volumes, and networks
docker compose down --volumes --remove-orphans

# (Optional) Remove the built images
docker compose down --volumes --rmi all
```


## Files Explained

| File | Purpose |
|---|---|
| `docker-compose.yaml` | Defines all 7 services (Postgres, API server, scheduler, DAG processor, triggerer, init, FastAPI monitor) with shared environment variables for JWT, SMTP, and database config |
| `Dockerfile` | Extends `apache/airflow:3.0.2` with ML dependencies from `requirements.txt` |
| `requirements.txt` | Extra Python packages: `pandas`, `numpy`, `scikit-learn`, `uvicorn`, `requests` |
| `.env` | User-configurable credentials (Airflow login, SMTP email/password) |
| `dags/main.py` | The Airflow DAG definition — 7 tasks chained with `>>` dependency operator |
| `dags/src/model_development.py` | Five ML functions (load, preprocess, split, train, evaluate) called by `PythonOperator` tasks |
| `dags/fastapi_monitor.py` | Standalone FastAPI app that authenticates with Airflow via JWT and exposes pipeline status endpoints |
| `dags/templates/success.html` | Green success page rendered at `/status` when the pipeline passes |
| `dags/templates/failure.html` | Red failure page with troubleshooting steps rendered when the pipeline fails |
| `dags/data/breast_cancer.csv` | Breast Cancer Wisconsin dataset (569 rows, 30 features + target) |


## Credits

- **Lab Credits:** [Prof. Ramin Mohammadi](https://www.mlwithramin.com/) — MLOps Course, Northeastern University
- **Original Lab:** [Airflow Lab 2](https://github.com/raminmohammadi/MLOps/tree/main/Labs/Airflow_Labs/Lab_2)
- **Modified by:** Sunny Yadav — Replaced Flask with FastAPI, migrated to Airflow 3.x + Docker Compose, resolved JWT auth issues